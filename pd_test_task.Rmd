---
title: "Pipedrive test task"
author: "Siim PÃµldre"
date: "12 5  2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE}
library(tidyverse)
library(psych)
library(kableExtra)
library(reticulate)
library(scales)
library(RColorBrewer)
library(corrplot)
library(Hmisc)
library(survival)
library(survminer)
```

```{python}
import warnings
warnings.filterwarnings("ignore")
```

The outline of the work is:  

1) Getting the data
2) Data tranformations and feature engineering  
3) General descriptives    
4) Survival analysis     
5) Clustering    
6) XGBoost     

I chose a dataset from kaggle that deals with a broadband companys churn. The dataset is from january 2020. I chose this specific dataset firstly because of its content. It is a subscription based service that offers several packagesbBandwidth) and has to deal with churn. This is similar to the types of data Pipedrive probably deals with and churn is a problem in any company. Secondly the features have nice metadata so i understand more clearly what i am looking at. Thirdly the data is not in a perfect condition so i have to do some data manipulation and feature engineering which is always fun.

I will use the kaggle API to download the dataset. 
```{bash}
kaggle datasets download jitendragoyal/broadband-customers-base-churn-analysis -f bbs_cust_base_scfy_20200210.csv

unzip -q bbs_cust_base_scfy_20200210.csv
```

```{r}
churn_tr <- read.csv("bbs_cust_base_scfy_20200210.csv")
```


In the dataset each customer has a row for each month they are ~500k rows alltogether.
There are:    
- 14 categorical variables    
- 5 integers (more caution in interpreting descriptives as numeric)  
- 1 numeric variables  
```{python}
import pandas as pd
r.churn_tr.info()
```

## Data transformations and feature engineering

Some notes:  
1)Since the dataset has a multiple rows for the same person (for each month) the whole dataset has to be reconfigured to have one row per customer.  
2) There are some missing values but the percentage of those is very low (under 1%) so they are not probably a major cause for concern when i remove them    
3) Some variables have 0 variance, and for some the metadata says that they can be ignored so i will remove those too  
4) The dataset seems to be built in a way where the people whos contract isn't over yet have rows for tenure for the full contract duration (even to the future). I have to remove from those people the number of rows that are in the future. This will keep them in the data as survivors (because they havent churned up until this point) but remove speculation about their future status as non-churners.    
5) I am using log total revenue produced by each customer as a feature    
6) I will use total complaints for each customer    
7) I Will not use contract dates, current_mth_churn, ce_expiry (features extracted from them are used instead), because they don't seem to give any additional information.   
8) Bandwidth will be coded as an ordinal variable becaus each package can be seen as "better" or "higher" than the previous one and it makes interpretation easier.  
9) A Variable is extracted from the bandwidth variable that tells if a person is using FTTO service or not  
```{r}
churn_oc0 <- churn_tr %>%
  drop_na() %>% #drop NA-s
  select(-bill_cycl & -serv_type & -line_stat) %>% #removing uninformative variables
  filter(ce_expiry < 0) %>%  #remove all customers who contract expiry is in the future
  add_row(churn_tr %>% #put back some rows (up until data collection point) from the customers who i removed
    filter(ce_expiry >= 0) %>%
    select(-bill_cycl & -serv_type & -line_stat) %>% 
    group_by(newacct_no) %>% 
    filter(row_number() < contract_month-ce_expiry)) %>% 
  filter(complaint_cnt %in% c("0","1", "2", "3", "4", "5", "6", "7")) %>% 
  mutate(complaint_cnt = as.numeric(complaint_cnt)) %>% 
  group_by(newacct_no) %>% 
  summarise(churn = first(churn), #is churned or not
            term_read_code = last(term_reas_code), #termination reason code
            term_reas_desc = last(term_reas_desc), #termination reason description  
            tenure = (max(tenure)-min(tenure)+1), #Tenure
            contract_month = first(contract_month), #contract length
            ce_expiry = first(ce_expiry), #difference between contract and jan 2020
            secured_revenue = sum(secured_revenue), #total revenue
            last_band = last(bandwidth), #last known bandwidth
            complaints= sum(complaint_cnt), #total complaints
            phone_serv = last(with_phone_service)) %>% #Uses phone servive or not
  mutate(ftto = case_when(
    str_detect(last_band, "FTTO") ~ 1,
    TRUE ~ 0
  )) %>%  #wether has FTTO service or not
  select(-newacct_no & -term_read_code & -term_reas_desc, -ce_expiry) %>% 
  filter(!secured_revenue <= 0) %>%  #removing ~400 people whose revenue is negative or 0
  mutate(log_revenue = log(secured_revenue)) %>%  #log secured revenue
  select(-secured_revenue)
churn_oc <- churn_oc0 %>% 
  mutate(last_band = recode(last_band, `BELOW 10M` = 0, `10M` = 1, `30M` = 2, `50M` = 3, `100M` = 4, `100M (FTTO)` = 5, `300M (FTTO)` = 6, `500M (FTTO)` = 7, `1000M (FTTO)` =8)) %>% #bandwidth package
  mutate(churn = recode(churn, Y = 1, N = 0)) %>%  #churn as numeric
  mutate(phone_serv = recode(phone_serv, Y = 1, N = 0)) #phone service as numeric
```

We are left with a dataset of 26334 unique customers and 8 features.    
7 floats,    
1 integer  
```{python}
r.churn_oc.info()
```

## Descriptive statistics

Here are some descriptive statistics for selected variables
```{r}
churn_oc %>%
  select(tenure, contract_month, complaints, log_revenue) %>% 
  summary()
```


Percentage of people churned is a lot lower so a there is quite a bit of class imbalance. I am going to use oversampling when running the xgboost classifier to mediate this problem.  
```{r}
churn_oc %>%
  mutate(churn = recode(churn, `1` = "Y", `0` = "N")) %>%
  ggplot(aes(x = as.factor(churn), fill = as.factor(churn))) +
  geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels = percent) +
  geom_text(aes(y = ((..count..)/sum(..count..)), label = scales::percent((..count..)/sum(..count..))), stat = "count", vjust = -0.25) +
  theme_bw()+
  theme(axis.text.y=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")+
  scale_fill_brewer(palette = "Set1")+
  xlab("Churn")+
  labs(title = "Proportion of people churning")
```

From visual assessment we can see some slight patterns like    
1)People not using a phone service are more likely to churn    
2)People using lower bandwidth packages are more likely to churn    
3)People using FTTO type bandwidth packages are less likely to churn
```{r}
churn_oc0 %>%
  select(churn, last_band, phone_serv, ftto) %>%
  rename(bandwidth = "last_band", `Uses phone service` = "phone_serv") %>% 
  mutate(ftto = recode(ftto, `1` = "Y", `0` = "N")) %>% 
  pivot_longer(!churn, names_to = c("feature"), values_to = c("value")) %>% 
  ggplot(aes(x = as.factor(value), fill = as.factor(churn)))+
  geom_bar(position = "fill") +
  theme_minimal()+
  coord_flip()+
  scale_fill_brewer(palette = "Set1")+
  facet_wrap(~feature, scales = "free")+
  labs(title = "Relationship between churn and some categorical variables")+
  xlab("Value")+
  labs(fill = "Churn")
```

## Survival analysis

Here I will do a quick survival analysis with an general overview about wether churn changes at some points of clients tenure and also how different variables are related to that. The model will allow us to create an extra feature named risk (cox hazard ratio) that we can use in classification.  

Observations about the survival model:  
1)phone service subsription has the biggest effect on churn with survival being higher for phone service users  
2)log_revenue is an important effect with higher revenue producers having lower churn (higher survival)  
2)bandwidth package seems to also be quite important with higher bandwidth meaning higher (on average 19% higher for every subsequent subscription) survival  
3)the survival curve shows that at around 23 months of tenure 50% of customers have churned and at around 18 months of tenure survival starts droping a lot faster  
```{r}
coxmod1 <- coxph(Surv(tenure, churn) ~ contract_month  + last_band +complaints + phone_serv + ftto + log_revenue, data = churn_oc)
exp(coxmod1$coefficients)
ggsurvplot(survfit(coxmod1), data = churn_oc, 
           censor = F, 
           surv.median.line = "hv", 
           ggtheme = theme_minimal())
churn_oc <- churn_oc %>% 
  bind_cols(risk = predict(coxmod1, churn_oc, type = "risk")) #adding risk as a feature to original dataset
```

## K-means clustering

```{r}
#removing risk and churn for clustering
churn_km <- churn_oc %>% 
  select(-churn) 
```

```{python}
#scaling the variables
import numpy as np
import pandas as pd 
from sklearn.preprocessing import scale
churn_km = pd.DataFrame(scale(r.churn_km), index = r.churn_km.index, columns = r.churn_km.columns)
```

I found 3 clusters to be the most informative
```{python}
from sklearn.cluster import KMeans
modelk = KMeans(n_clusters=3, random_state=203)
modelk.fit(churn_km)
churn_km['kclusters'] = modelk.predict(churn_km) #standardized dataset
```

```{python}
churn_ock = r.churn_oc
churn_ock['kclusters'] = churn_km['kclusters'] #original dataset with clusters added as a feature
```


```{python}
#A quick hack i needed to use because using Rmarkdown the matplotlib wasnt clearing the plot #space and showing wrong plots in wrong places. This code can be ignored.
import seaborn as sns1
import matplotlib.pyplot as plt1
fig, ax = plt1.subplots(figsize=(0.1,0.1))
plt1.style.use('ggplot')
plt1.show()
```

Mapping the clusters to some of the continuous variables (Note: the labels of the clusters change even with random seed so "other cluster" just means a cluster not refering to a specific label) , we can see that  
1) Inone of the clusters are people who use a lower bandwidth subscription but teir log total revenue is on the higher side.  Looking at the second plot these are also the people with lower longer tenure.   
3) In an other cluster are people who use a lower bandwidth subscription with also a lower log total revenue. On the second plot we can see that these people also have a higher risk of churn.
4) In a third cluster are people who use a higher bandwidth subscription but their log total revenue is varied and also their tenure is varied.  

```{python}
import seaborn as sns1
import matplotlib.pyplot as plt1
plt1.style.use('ggplot')
a=sns1.jointplot(x='log_revenue', y='last_band', data=churn_ock, hue='kclusters', palette='Set1')
a.fig
```

```{python}
b=sns1.jointplot(x='tenure', y='risk', data=churn_ock, hue='kclusters', palette='Set1')
b.fig
```

Looking at churn in different clusters we can see that it is only slightly different so the cluster feature is not likely to be very important in the classifier algorithm.

```{r}
py$churn_ock %>%
  as_data_frame() %>%
  mutate(churn = recode(churn, `1` = "Y", `0` = "N")) %>%
  ggplot(aes(x = as.factor(kclusters), fill=churn)) + 
  geom_bar(position = "fill") +
  theme_minimal()+
  scale_fill_brewer(palette = "Set1")+
  labs(title = "Relationship between churn and clusters")+
  xlab("Value")+
  labs(fill = "Churn")
```

## XGBoost

```{python}
import matplotlib.ticker as mtick 
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics
import matplotlib.cm as cm
from sklearn.preprocessing import normalize
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.model_selection import train_test_split
from xgboost import plot_importance
```

```{python}
features = churn_ock.loc[:, churn_ock.columns != 'churn'].columns
scaler  = MinMaxScaler(feature_range=(0,1))
scaler.fit(churn_ock.loc[:, churn_ock.columns != 'churn'])
churn_ockx = pd.DataFrame(scaler.transform(churn_ock.loc[:, churn_ock.columns != 'churn']))
churn_ockx.columns = features
```

```{python}
churn_ockx['churn'] = r.churn_oc['churn']
churn_ockx['churn'] = churn_ockx['churn'].astype(int)
```

Train test split
```{python}
y = churn_ockx['churn'].values
x = churn_ockx.drop(columns = ['churn'])
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
```

Using a simple random over sampling method here to deal with class imbalance. SMOTE can not be used because it makes non-sensical samples from the minority class if categorical predictor variables are present. SMOTE-NC is another option that can be used but there are some variables in this dataset that are not strictly categorical but also not strictly nominal (contract_month, last_band). More concideration should be given to how to code these variables in a way where synthesizing new data wouldn't create datapoints that do not make sense.
```{python}
from imblearn.over_sampling import RandomOverSampler
oversampler = RandomOverSampler(random_state=0)
x_train_o, y_train_o = oversampler.fit_resample(x_train, y_train)
```

As said i will use xgboost for classification. For no particular reason other than it is a decent classifier. In practice other classifiers should also be used and results compared to find the best one. I will use kfold validation and a small grid-search to find the best combination of selected hyperparameters. The grid search code chunk will not be evalated because to knit the html document in Rmarkdown i would have to wait for it to train again (~20 mins) and sometimes my computer crashes. I will just run it once and take the best combinattion of hyperparameters that the model finds and enter them to a single xgboost below.

The grid search code looks like this:
```{r eval = FALSE}
from imblearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
params = {'max_depth': [3, 4, 5],
          'min_child_weight': [1, 5, 10],
          'gamma': [0.5, 1 ,1.5 ,2,5],
          'colsample_bytree': [0.6 ,0.8 ,1.0],
          'learning_rate': [0.05, 0.08, 0.1, 0.13],
          'objective': ['binary:logistic'],
          'random_state': [34]}
      
kf = KFold(n_splits=5, shuffle=False)
    
grid = GridSearchCV(xgb.XGBClassifier(), params, cv = kf, refit  =True, scoring = 'f1', return_train_score=True, n_jobs=4)
grid.fit(x_train_o, y_train_o)
```

The best parameters are entered into the classifier here. On the validation set the f-score was 0.56. Thats allready quite low so im not expecting anything great from the test set either. 
```{python}
xgb_model = xgb.XGBClassifier(max_depth=5, gamma = 2, learning_rate=0.13, min_child_weight = 1, colsample_bytree=1.0, objective= 'binary:logistic',n_jobs=-1, random_state=34).fit(x_train_o, y_train_o)
prediction_test = xgb_model.predict(x_test)
```


```{python}
accuracy = xgb_model.score(x_test[x_train_o.columns], y_test)
f1_score = metrics.f1_score(y_test, prediction_test)
precision = metrics.precision_score(y_test, prediction_test)
recall = metrics.recall_score(y_test, prediction_test)

xg_results = pd.DataFrame({'metric': ['accuracy', 'f1 score', 'precision', 'recall'],
                            'value': [accuracy, f1_score, precision, recall]})
```

Looking at the results we can see that our accuracy is quite low in the test set at 0.67. Recall is okay with catching 67% of churners from all churners. Altho at the same time we can see that precision is very low with only 37% of predicted churner actually churning. F1 score sums that up with a low 0.46. Our model is not performing well here some of it probably has to to with the underlying data also. It has quite a few features. It could do well with some demographic details about the customers for better performance.
```{r}
py$xg_results %>% 
  mutate(value = round(value, 2)) %>% 
  ggplot(aes(x=metric, y = value, fill = metric))+
  geom_bar(stat = "identity")+
  geom_text(aes(label=value), position=position_dodge(width=0.9), vjust=-0.25)+
  theme_minimal()+
  scale_fill_brewer(palette = "Set1")+
  theme(axis.text.y=element_blank(),
        axis.title.y=element_blank())+
  labs(title = "XGBoost scores")+
  xlab("Metric")+
  labs(fill = "Value")+
  theme(legend.position = "none")
```

lastly we can look at feature importance. Log revenue is our top predictor as was also seen in the survival model. Altho here we can see that phone_serv is not a very good predictor. The eature risk that we extracted from the survival model is important and also incorporates phone_serv allready. Tenure is also important. Cluster assignment is not important at all tho. Some of these features could be safely removed probably and maybe improve the model like this. 
```{python}
import matplotlib.pyplot as plt3
plt3.style.use('ggplot')
fig, ax = plt3.subplots(figsize=(10,8))
xg_b=plot_importance(xgb_model, ax=ax);
plt3.tight_layout()
plt3.show(xg_b)
```
