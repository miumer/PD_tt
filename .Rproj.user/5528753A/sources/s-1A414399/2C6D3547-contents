---
title: "Pipedrive test task"
author: "Siim PÃµldre"
date: "5 5 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE, message = FALSE}
library(tidyverse)
library(psych)
library(kableExtra)
library(patchwork)
library(pheatmap)
library(reticulate)
library(scales)
library(RColorBrewer)
library(corrplot)
library(Hmisc)
library(survival)
library(survminer)
```

TRY TO AUTOMAICALLY ACCESS FILE FROM KAGGLE.

1)Data tranformations
2)General descriptives
3)Logistic regression on raw data
4)Clustering and cluster analsys (how clusters differ + logistic regression using clusters with baseline)
5)XGBoost and feature analysis

I chose a dataset from kaggle that deals with a broadband companies churn. The dataset is from january 2020. I chose this specific dataset firstly because of its content. It is a subscription based service that offers several packages(Bandwidth) and has to deal with churn. This is similar to the types of data Pipedrive probably deals with and churn is a problem in any company. Secondly the features have nice metadata so i understand more clearly what i am looking at. The interesting thing with this dataset is that each customer has a row for each month they are a customer (~500k rows alltogether) which is good for survival analysis but for rest of the analysis i am planning to do it is better to have the data in a format where each customer has one row. For this i will transform the dataset.
```{r}
churn_tr <- read.csv("bbs_cust_base_scfy_20200210.csv", header = TRUE)
```

There are some missing values but for two features the percentages are very low (under 1%) so they are not probably a major cause for concern when we remove these lines.
```{r}
Puuduvad <- churn_tr %>% 
  summarise_all(funs(sum(is.na(.)))) %>% 
  pivot_longer(everything(), names_to = "Feature", values_to = "Missing") 

Puuduvad %>% 
  mutate(Prop.missing = round(`Missing`/nrow(churn_tr),4)) %>%
  filter(Missing > 0) %>% 
  arrange(., desc(Missing)) %>% 
  kbl() %>% 
  kable_styling()

churn_tr <- churn_tr %>% 
  drop_na()
```

There are:  
- 14 categorical variables  
- 5 integers (more caution in interpreting descriptives as numeric)
- 1 numeric variables

The data has ~510k rows and 20 columns. Secured revenue is the only numeric/float variable. I will treat image and contract month as categorical variables (altho they are ordinal). Image because it is actually a date variable about billing month and contract month because it has a more categorical character being a contract type or category.
```{python}
r.churn_tr.info()
```

Looking at variables to see if some have 0 variance to remove them:
1) bill_cycl has no variance so we don't need that variable (labeled as ignorable in the data source too)
2)serv_type also has only one value(BBS)
```{r}
churn_tr %>%
  lapply(unique) %>% 
  lapply(length)
```

Some variables are not explained and the metadata says that they can be ignored. I will remove some of them here. Two of them have 0 variance as said. 
```{r}
churn_tr <- churn_tr %>% 
  select(-bill_cycl & -serv_type & -line_stat) #getting rid of variable with no variance
```

Since the dataset seems to be built in a way where the people whos contract isn't over yet, are put down for tenure for the full contract duration (even in the future as expected) i have to remove from those people the number of rows that predict them staying for the full duration. THis will keep them in the data as survivors (because they havent churned up until this point).
```{r}
churn_tr <- churn_tr %>% 
  filter(ce_expiry >= 0) %>% 
  group_by(newacct_no) %>% 
  filter(row_number() < contract_month-ce_expiry)
```

```{r}
churn_tr %>% 
  ggplot(aes(x = churn,y = ce_expiry, fill = churn)) 
  geom_boxplot()
```

```{r}
churn_oc %>% 
  filter(tenure < contract_month)
```


```{r}
churn_tr %>% 
  filter(newacct_no == "70082185.001.000000083")
```


Putting the dataset in more workable format and doing some feature engineering below. 

Some notes:  
1) Since the dataset seems to be built in a way where the people whos contract isn't over yet, are put down for tenure for the full contract duration i have to remove from those people the number of rows that show the futuren. This will keep them in the data as survivors (because they havent churned up until this point).  
1) Going to use total revenue produced for each customer as a feature  
3) I will use total complaints for each client  
4) I Will not use variables image, contract dates (features extracted from them are used), and current_mth_churn, because they don't seem to give any additional information  
5) Bandwidth will be coded as ordinal variable because it can be concidered a variable where each package is "better" or "higher" than the previous one  
```{r}
churn_oc <- churn_tr %>%
  filter(ce_expiry < 0) %>% #remove all customers who contract expiry is in the future
  add_row(churn_tr %>% #put back some rows (up until data collection point) from the customers who i removed
    filter(ce_expiry >= 0) %>% 
    group_by(newacct_no) %>% 
    filter(row_number() < contract_month-ce_expiry)) %>% 
  filter(complaint_cnt %in% c("0","1", "2", "3", "4", "5", "6", "7")) %>% 
  mutate(complaint_cnt = as.numeric(complaint_cnt)) %>% 
  group_by(newacct_no) %>% 
  summarise(churn = first(churn), #is churned or not
            term_read_code = last(term_reas_code), #termination reason code
            term_reas_desc = last(term_reas_desc), #termination reason description  
            tenure = (max(tenure)-min(tenure)+1), #Tenure
            contract_month = first(contract_month), #contract length
            ce_expiry = first(ce_expiry), #difference between contract and jan 2020
            secured_revenue = sum(secured_revenue), #total revenue
            last_band = last(bandwidth), #last known bandwidth
            complaints= sum(complaint_cnt), #total complaints
            phone_serv = last(with_phone_service)) %>% #Last info about using phone service
  mutate(ftto = case_when(
    str_detect(last_band, "FTTO") ~ 1,
    TRUE ~ 0
  )) %>% #wether has FTTO service or not
  mutate(last_band = recode(last_band, `BELOW 10M` = 0, `10M` = 1, `30M` = 2, `50M` = 3, `100M` = 4, `100M (FTTO)` = 5, `300M (FTTO)` = 6, `500M (FTTO)` = 7, `1000M (FTTO)` =8)) %>% #bandwidth package
  mutate(churn = recode(churn, Y = 1, N = 0)) %>%  #churn as numeric
  mutate(phone_serv = recode(phone_serv, Y = 1, N = 0)) %>%  #phone service as numeric
  select(-newacct_no & -term_read_code & -term_reas_desc)

```

We are left with a dataset of 26763 unique customers and 9 features.  
7 floats,  
3 integers  
```{python}
r.churn_oc.info()
```

Here we will do a quick survival analysis with an general overview about wether churn changes at some points of clients tenure and also how different variables have an effect on that. The main goal is to extract a cox hazard ratio score for clients as a feature to use in the xgboost classifier. With more space (and time) different types of scores could be extracted and tested, but here we will stick with the cox hazard ratio. 

The survival curve shows us that survival probability goes down pretty linearly as tenure grows. Around a year 
```{r}
coxmod1 <- coxph(Surv(tenure, churn) ~ contract_month  + last_band +complaints + phone_serv + ftto + secured_revenue + ce_expiry, data = churn_oc)
summary(coxmod1)

ggsurvplot(survfit(coxmod1), data = churn_oc, 
           censor = F, 
           surv.median.line = "hv", 
           ggtheme = theme_minimal())

churn_oc <- churn_oc %>% 
  bind_cols(risk = predict(coxmod1, churn_oc, type = "risk"))
```


Percentage of people churned is a lot lower so a there is quite a bit of class imbalance. I am going to use oversampling to mediate this problem.
```{r}
churn_oc %>% 
  ggplot(aes(x = as.factor(churn), fill = as.factor(churn))) +
  geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels = percent) +
  geom_text(aes(y = ((..count..)/sum(..count..)), label = scales::percent((..count..)/sum(..count..))), stat = "count", vjust = -0.25) +
  theme_minimal()+
  theme(axis.text.y=element_blank(), 
        axis.ticks=element_blank(),
        axis.title.y=element_blank(),
        legend.position = "none")+
  scale_fill_brewer(palette = "Set1")
```

!!!!!!!!!!!!!!!!WORK ON THESE!!!!!!!!!!!!!!!!

Some observations from descriptive statistics:
1) Most people have used 1 - 2 services
2) Average tenure is around 17.54 with median at 23. For 50% of people its between 12 and 23
3) Contracts expire on average in 5.8 months (can also be negative if expired before jan 2020)
4) Average secured revenue per month is 550 on average with a median on 226. It is highly skewed.
5) Average number of complaints per month is 0 but maximum is 5 (while actual maximum complaints in a single month is 7)
```{r}
churn_oc %>%
  select_if(~class(.) != 'character') %>%
  describe(., IQR = TRUE,quant = c(.25, .75)) %>%
  kbl() %>% 
  kable_styling()
```

From visual assessment we can see that: 
1)people not using a phone service are more likely to churn
2)people using lower bandwidth packages are more likely to churn
```{r}
churn_oc %>%
  select(-term_read_code, -term_reas_desc) %>%  #uninformative variables in this case
  select_if(~class(.) == 'character') %>%
  pivot_longer(!churn, names_to = c("feature"), values_to = c("value")) %>% 
  ggplot(aes(x = value, fill = churn))+
  geom_bar(position = "fill") +
  theme_minimal()+
  coord_flip()+
  scale_fill_brewer(palette = "Dark2")+
  facet_wrap(~feature, scales = "free")+
  labs(title = "Relationship between churn and some categorical variables")
```

Now we will use kmeans clustering for feature generation for the xgboost model.
```{r}
#removing not needed columns
churn_km <- churn_oc %>% 
  select(-churn) 
```

```{python}
#scaling the variables
import numpy as np
import pandas as pd 
from sklearn.preprocessing import scale
churn_km = pd.DataFrame(scale(r.churn_km), index = r.churn_km.index, columns = r.churn_km.columns)
```

I found 4 clusters to be the most informative
```{python}
from sklearn.cluster import KMeans
modelk = KMeans(n_clusters=4)
modelk.fit(churn_km)
churn_km['kclusters'] = modelk.predict(churn_km)
```

!!!!!!!!!!!!WORK ON THEESEEE!!!!!!!!!!!!!!!!!!!!

Mapping these clusters onto our variables we can observe that:
1) People who have churned most belong to cluster number 1 while people who have not churned belong to clusters  0 and 2 
2) Cluster number 1 is also lower on tenure and (makes sense because churners will have lower tenure) and lower on contract expiry (also makes sense because churners contracts are more likely to be expired logically). In cluster 1 many more people also have phone service than don't have it. 
2) Cluster 2 is higher in tenure and also higher on contract length notably. In cluster number 2 most people don't have phone service. 
3) In cluster 0 most people have phone service

```{python}
churn_km["kclusters"].unique()
```

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
sns.jointplot(x='risk', y='last_band', data=churn_km, hue='kclusters', palette='Set1')
plt.show()
```

```{python}
sns.jointplot(x='ce_expiry', y='contract_month', data=churn_km, hue='kclusters', palette='Dark2')
plt.show()
```

WHY IS THIS NOT WORKING?????

```{r}
churn_oc %>% 
  cbind(py$churn_km$kclusters) %>%
  select(-term_read_code, -term_reas_desc) %>%  #uninformative variables in this case
  mutate(kclusters = as.character(`py$churn_km$kclusters`)) %>% 
  select_if(~class(.) == 'character') %>%
  pivot_longer(!kclusters, names_to = c("feature"), values_to = c("value")) %>% 
  ggplot(aes(x = value, fill =kclusters))+
  geom_bar(position = "fill") +
  theme_minimal()+
  coord_flip()+
  scale_fill_brewer(palette = "Dark2")+
  facet_wrap(~feature, scales = "free")+
  labs(title = "Relationship between churn and some categorical variables")
```

Lastly i will run xgboost to and predict churn
```{python}
import matplotlib.ticker as mtick 
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
import matplotlib.cm as cm
from sklearn.preprocessing import normalize
import xgboost as xgb
from sklearn.model_selection import train_test_split
from xgboost import plot_importance

plt.style.use('ggplot')
```

```{python}
churn_dummies = pd.get_dummies(churn_km) #one-hot encoding
```

```{r}
churn_dummies <- py$churn_dummies %>% 
  bind_cols(churn = churn_oc$churn)

cor(as.matrix(churn_dummies))
corrplot(cor(as.matrix(churn_dummies)))
```

```{python}
features = r.churn_dummies.loc[:, r.churn_dummies.columns != 'churn'].columns

scaler  = MinMaxScaler(feature_range=(0,1))

scaler.fit(r.churn_dummies.loc[:, r.churn_dummies.columns != 'churn'])

churn_dummies = pd.DataFrame(scaler.transform(r.churn_dummies.loc[:, r.churn_dummies.columns != 'churn']))

churn_dummies.columns = features
```

SIIIT EDASI



```{python}
y = r.churn_dummies['churn']
x = churn_dummies.drop(columns = ['churn'])

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
```

```{python}
xgb_model = xgb.XGBClassifier(max_depth=5, gamma = 0.1 , learning_rate=0.08, objective= 'binary:logistic',n_jobs=-1, random_seed=0).fit(x_train, y_train)

prediction_testx = xgb_model.predict(x_test)

print('Accuracy of XGB classifier on training set: {:.2f}'
       .format(xgb_model.score(x_train, y_train)))

print('Accuracy of XGB classifier on test set: {:.2f}'
       .format(xgb_model.score(x_test[x_train.columns], y_test)))
```

```{python}
fig, ax = plt.subplots(figsize=(10,8))
plot_importance(xgb_model, ax=ax);
plt.tight_layout()
plt.show()
```

